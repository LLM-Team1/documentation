## RAG Evaluation {#rav-evaluation}

# Fundamental Goals of Evaluation


# Methods for RAG-Evaluation

There are different options when it comes to evaluating a RAG-System. After looking into the literature we found
different out that there is not the one perfect method to evaluate a RAG-System.

In Information Retrieval (IR) Venable
et al. (2016) looks at the whole development process of an IR System. In the following Figure XX we see different
approaches for different products. Our product falls in the "Human Risk & Effectiveness" path. This path is made for
solutions that are human centered just as our system. The black triangles are symbolizing different possible evaluation
steps. On the y axis we find a scale coming from artificial, going to more naturalistic. Artificial stands for a more
automated evaluation process. On the other hand we find a scale from formative to summative on the x axis. Formative
means that with this evaluation we try to improve the output; while summative evaluations should give an more generall
overview over the final product and whether it is providing all the functionalities it should provide as a solution
a customer could use.

![Framework for Evaluation in Design Science Research (FEDS) [Venable et. al, 2016]](images/FEDS_Venable_et_al_2014.png)

Since our product is still in the prototype phase we concentrated our evaluation on the first three triangles. Meaning
we did an automated evaluation using libraries like TruLens and RAGAS. The biggest advantage coming with these
techniques is that they are comparably cheap (Venable et al., 2016). Next we used human evaluation to prove the results
of our automated evaluation approach with this human centered approach. Human centered approaches for evaluation bring
the advantage to give a more realistic view on the system. Problem with these approaches is that they are comparably
expensive and time consuming (Venable et al., 2016)

# General Evaluation Foundation

We used our evaluation methods always on both, the basic RAG and our customized RAG. To do so we handed over the
following eight questions. On the one hand question 1,2,3,5 are more "simple" questions, aiming to find a simple answer. Question
4,6,7,8 on the other hand are more "complex" questions, aiming to find a solution, where multiple contexts are needed
from the business report.

*Prompts to RAG in evaluation process:*

1. What product categories and car models does Mercedes-Benz offer?
2. What was the total revenue of Mercedes-Benz in the last fiscal year?
3. Which new markets did Mercedes-Benz enter in the past year?
4. What specific sustainability goals has Mercedes-Benz set for the upcoming year?
5. How much did Mercedes-Benz invest in research and development in the last fiscal year?
6. What are the key challenges Mercedes-Benz has faced in its supply chain and production processes, and how have these
impacted its operational efficiency?
7. How is Mercedes-Benz adapting its overall business strategy to align with current global automotive trends, such as
digitalization, sustainability, and changing consumer preferences?
8. What opportunities does Mercedes-Benz have in the realm of electric vehicles and autonomous driving technologies, and
what strategic initiatives are they undertaking in these areas?


# Automated Evaluation Approach

As already mentioned we used different libraries to evaluate our RAG-System.

For the evaluation of our RAG application, we started to use “The RAG Triad” designed by Team TruEra (2023). The RAG triad
consists of the following three main components: query, the context, and the response. Its goal is to
evaluate the RAG system to make it hallucination free.

![The RAG Triad [Team TruEra, 2023]](images/The_RAG_Triad_TruLens_2023.png)

These three main evaluation components are connected to each other and therefore result in an evaluation cycle that
continues over the lifecycle of a RAG application. The following describes the three evaluation stages of the RAG Triad.

*## Context Relevance*

The first evaluation component is the context relevance. In this step the goal is to evaluate if the context of the
answer - provided by the RAG app - is relevant to the query.

*##Groundedness*

In the second step the goal is to evaluate the groundedness of the RAG app. The answer (which is relevant to the
query) may lack groundedness. This means that the answer is not supported by the retrieved context but is generated
from the LLMs training data. This training data is unverified,  inappropriate, and often irrelevant for the use in
this RAG application. In this evaluation step the answer by the RAG app is compared with the verified and trusted
data stored in the vector database.

*##Answer Relevance*

In the last step of the evaluation cycle the goal is to evaluate the value a user gets and if the Response answers
the original question of the user. Even if the answer contains to the query relevant information and the answer
consists of data stored in the vector database the RAG response can still fail to answer the users question. By
measuring the relevance of the response to the original question/ query the relevance of the response can be verified.

The TruLens library offers a dashboard to overview the criteria mentioned above as you improve the junk sizes, prompts
etc. Since this process makes it harder to compare the results of earlier evaluations or our human evaluation we decided
to use another RAG evaluation library: RAGAS.

## RAGAS Evaluation

Finally we ended up using the RAGAS framework since this made it easy to extract the evaluation results in an excel
file to later compare it with our human centered evaluation approach. Following you we see the results of our evaluation
using the RAGAS framework.

In the following is the overall performance of the Base RAG and our Custom RAG, using the different chunk sizes. We found
that the best chunk size for the Base RAG is 700; while the best chunk size for our Custom RAG is a size of 900.

![Comparison of overall RAGAS Evaluation](images/Comparison_RAG_RAGAS_Evaluation.png)

Our Custom RAG outperformed the Base RAG

# Human Centered Evaluation Approach

For our human evaluation we let three human evaluators evaluate the in total 96 Results of the Basic RAG and the Custom
RAG.

We used three criteria: "Relevance and Accuracy", "Clarity and Coherence", and "Completeness and Depth". The following
tabel shows the definition on our evaluation scale for the human evaluation.

![Criteria Tabel for Human Evaluation](images/Criteria_table_Human_Evaluation.png)

The Results for the human evaluation show, that the best chunk size for the Base RAG is 700, while the best chunk size
for our Custom RAG is 900. Overall our Custom RAG performs better than the Base RAG on this questions, when it comes
to human perception

![Comparison of overall Human Evaluation](images/Comparison_RAG_Human_Evaluation.png)


# Further Naturalistic & Formative Evaluation for Prompt Engineering

We also used another naturalistic and formative evaluation (Venable et. al, 2016) for further prompt engineering in
our SWOT analysis. In our use case we have the advantage of having predefined prompts for our SWOT-analysis. That makes
it possible to evaluate and adjust the exact used prompts in our tool.

# Future Evaluation

- Live evaluation of generated results, to ensure quality during use of the tool
- If there are more resources available the human evaluation can be made with more than just three evaluators