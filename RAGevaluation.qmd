## RAG Evaluation {#rav-evaluation}

# Fundamental Goals of Evaluation


# Methods for RAG-Evaluation

There are different options when it comes to evaluating a RAG-System. After looking into the literature we found
different out that there is not the one perfect method to evaluate a RAG-System.

In Information Retrieval (IR) Venable
et al. (2016) looks at the whole development process of an IR System. In the following Figure XX we see different
approaches for different products. Our product falls in the "Human Risk & Effectiveness" path. This path is made for
solutions that are human centered just as our system. The black triangles are symbolizing different possible evaluation
steps. On the y axis we find a scale coming from artificial, going to more naturalistic. Artificial stands for a more
automated evaluation process. On the other hand we find a scale from formative to summative on the x axis. Formative
means that with this evaluation we try to improve the output; while summative evaluations should give an more generall
overview over the final product and whether it is providing all the functionalities it should provide as a solution
a customer could use.

![Framework for Evaluation in Design Science Research (FEDS) [@venable_feds:_2016]](images/FEDS_Venable_et_al_2014.png)

Since our product is still in the prototype phase we concentrated our evaluation on the first three triangles. Meaning
we did an automated evaluation using libraries like TruLens and RAGAS. The biggest advantage coming with these
techniques is that they are comparably cheap [@venable_feds:_2016]. Next we used human evaluation to prove the results
of our automated evaluation approach with this human centered approach. Human centered approaches for evaluation bring
the advantage to give a more realistic view on the system. Problem with these approaches is that they are comparably
expensive and time consuming [@venable_feds:_2016].

# General Evaluation Foundation

We used our evaluation methods always on both, the basic RAG and our customized RAG. To do so we handed over the
following eight questions. On the one hand question 1,2,3,5 are more "simple" questions, aiming to find a simple answer. Question
4,6,7,8 on the other hand are more "complex" questions, aiming to find a solution, where multiple contexts are needed
from the business report.

*Prompts to RAG in evaluation process:*

1. What product categories and car models does Mercedes-Benz offer?
2. What was the total revenue of Mercedes-Benz in the last fiscal year?
3. Which new markets did Mercedes-Benz enter in the past year?
4. What specific sustainability goals has Mercedes-Benz set for the upcoming year?
5. How much did Mercedes-Benz invest in research and development in the last fiscal year?
6. What are the key challenges Mercedes-Benz has faced in its supply chain and production processes, and how have these
impacted its operational efficiency?
7. How is Mercedes-Benz adapting its overall business strategy to align with current global automotive trends, such as
digitalization, sustainability, and changing consumer preferences?
8. What opportunities does Mercedes-Benz have in the realm of electric vehicles and autonomous driving technologies, and
what strategic initiatives are they undertaking in these areas?


# Automated Evaluation Approach

As already mentioned we used different libraries to evaluate our RAG-System.

For the evaluation of our RAG application, we started to use “The RAG Triad” designed by Team TruEra
[@noauthor_rag_nodate]. The RAG triad consists of the following three main components: query, the context, and the
response. Its goal is to evaluate the RAG system to make it hallucination free.

![The RAG Triad [@noauthor_rag_nodate]](images/The_RAG_Triad_TruLens_2023.png)

These three main evaluation components are connected to each other and therefore result in an evaluation cycle that
continues over the lifecycle of a RAG application. The following describes the three evaluation stages of the RAG Triad.

*Context Relevance*

The first evaluation component is the context relevance. In this step the goal is to evaluate the retrieved context and
its relevance to the query/ initial question. In our custom RAG we use query rewriting. Therefore this value can be
expected to be lower for our custom RAG. Our RAG finds the context based on the rewritten query; which leads to a lower
relevance of the discovered context to the initial question.

*Groundedness*

In the second step the goal is to evaluate the groundedness of the RAG app. The answer (which is relevant to the
query) may lack groundedness. This means that the answer is not supported by the retrieved context but is generated
from the LLMs training data. This training data is unverified,  inappropriate, and often irrelevant for the use in
this RAG application. In this evaluation step the answer by the RAG app is compared with the verified and trusted
data stored in the vector database.

*Answer Relevance*

In the last step of the evaluation cycle the goal is to evaluate the value a user gets and if the Response answers
the original question of the user. Even if the answer contains to the query relevant information and the answer
consists of data stored in the vector database the RAG response can still fail to answer the users question. By
measuring the relevance of the response to the original question/ query the relevance of the response can be verified.

The TruLens library offers a dashboard to overview the criteria mentioned above as you improve the junk sizes, prompts
etc. Since this process makes it harder to compare the results of earlier evaluations or our human evaluation we decided
to use another RAG evaluation library: RAGAS.

## RAGAS Evaluation

Finally we ended up using the RAGAS framework [@noauthor_metrics_nodate] since this made it easy to extract the evaluation results in an excel
file to later compare it with our human centered evaluation approach. Following you we see the results of our evaluation
using the RAGAS framework.

In the following we compare the overall performance of the Base RAG and our Custom RAG, using the different chunk sizes. We found
that the best chunk size for the Base RAG is 700; while the best chunk size for our Custom RAG is a size of 900.

![Comparison of overall RAGAS Evaluation](images/Comparison_RAG_RAGAS_Evaluation.png)

In this diagram we can see that our Custom RAG has an overall lower Context Precision. As expected this is caused by our
query rewriting. At the same time we notice that the answer relevance is higher; meaning we archived our goal to improve
the answer relevance by rewriting the initial question. Overall our Custom RAG at a chunk size of 900 outperformed the
base RAG at a chunk size of 700.

# Human Centered Evaluation Approach

For our human evaluation we let three human evaluators evaluate the in total 96 Results of the Basic RAG and the Custom
RAG.

We used three criteria: "Relevance and Accuracy", "Clarity and Coherence", and "Completeness and Depth"
[@shuster_retrieval_2021]. The following tabel shows the definition on our evaluation scale for the human evaluation.

![Criteria Tabel for Human Evaluation](images/Criteria_table_Human_Evaluation.png)

The Results for the human evaluation show, that the best chunk size for the Base RAG is 700, while the best chunk size
for our Custom RAG is 900. Overall our Custom RAG performs better than the Base RAG on this questions, when it comes
to human perception

![Comparison of overall Human Evaluation](images/Comparison_RAG_Human_Evaluation.png)


# Further Naturalistic & Formative Evaluation for Prompt Engineering

We also used another naturalistic and formative evaluation [@venable_feds:_2016] for further prompt engineering in
our SWOT analysis. In our use case we have the advantage of having predefined prompts for our SWOT-analysis. That makes
it possible to evaluate and adjust the exact used prompts in our tool.

# Future Evaluation

Next steps for the evaluation in our project is the live evaluation of generated results, to ensure high
quality during the use of our tool. If there are more resources available the human evaluation can be executed
with more than just three evaluators to make the results more meaningful. As discussed earlier, it could be helpful
to adjust the RAGAS metrics of the context relevance to measure the relevance of the retrieved context to the rewritten
query.