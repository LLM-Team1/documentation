## RAG Evaluation

# Fundamental Goals of Evaluation


# Methods for RAG-Evaluation

There are different options when it comes to evaluating a RAG-System. After looking into the literature we found
different out that there is not the one perfect method to evaluate a RAG-System.

In Information Retrieval (IR) Venable
et al. (2016) looks at the whole development process of an IR System. In the following Figure XX we see different
approaches for different products. Our product falls in the "Human Risk & Effectiveness" path. This path is made for
solutions that are human centered just as our system. The black triangles are symbolizing different possible evaluation
steps. On the y axis we find a scale coming from artificial, going to more naturalistic. Artificial stands for a more
automated evaluation process. On the other hand we find a scale from formative to summative on the x axis. Formative
means that with this evaluation we try to improve the output; while summative evaluations should give an more generall
overview over the final product and whether it is providing all the functionalities it should provide as a solution
a customer could use.

Since our product is still in the prototype phase we concentrated our evaluation on the first three triangles. Meaning
we did an automated evaluation using libraries like TruLens and RAGAS. The biggest advantage coming with these
techniques is that they are comparably cheap (Venable et al., 2016). Next we used human evaluation to prove the results
of our automated evaluation approach with this human centered approach. Human centered approaches for evaluation bring
the advantage to give a more realistic view on the system. Problem with these approaches is that they are comparably
expensive and time consuming (Venable et al., 2016)

# General Evaluation Foundation

We used our evaluation methods always on both, the basic RAG and our customized RAG. To do so we handed over the
following eight questions. On the one hand question 1,2,3,5 are more "simple" questions, aiming to find a simple answer. Question
4,6,7,8 on the other hand are more "complex" questions, aiming to find a solution, where multiple contexts are needed
from the business report.

*Prompts to RAG in evaluation process:*

1. What product categories and car models does Mercedes-Benz offer?
2. What was the total revenue of Mercedes-Benz in the last fiscal year?
3. Which new markets did Mercedes-Benz enter in the past year?
4. What specific sustainability goals has Mercedes-Benz set for the upcoming year?
5. How much did Mercedes-Benz invest in research and development in the last fiscal year?
6. What are the key challenges Mercedes-Benz has faced in its supply chain and production processes, and how have these
impacted its operational efficiency?
7. How is Mercedes-Benz adapting its overall business strategy to align with current global automotive trends, such as
digitalization, sustainability, and changing consumer preferences?
8. What opportunities does Mercedes-Benz have in the realm of electric vehicles and autonomous driving technologies, and
what strategic initiatives are they undertaking in these areas?


# Automated Evaluation Approach

As already mentioned we used different libraries to evaluate our RAG-System.


Finally we ended up using the RAGAS framework since this made it easy to extract the evaluation results in an excel
file to later compare it with our human centered evaluation approach. Following you we see the results of our evaluation
using the RAGAS framework.


# Human Centered Evaluation Approach

- Three evaluators
- Evaluating the RAG-results based on the three criteria: "Relevance and Accuracy", "Clarity and Coherence", and "Completeness and Depth"
- Presentation of results...