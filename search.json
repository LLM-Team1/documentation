[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "StratMystiqPro Documentation",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#hinweise",
    "href": "index.html#hinweise",
    "title": "StratMystiqPro Documentation",
    "section": "Hinweise",
    "text": "Hinweise\n\nMögliche Gliederung (Design Science Research)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#styling",
    "href": "index.html#styling",
    "title": "StratMystiqPro Documentation",
    "section": "Styling",
    "text": "Styling\n\nHTML Themes",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction and Project Overview",
    "section": "",
    "text": "In this module we got the task to identify a use case from a business model canvas to improve a business unit or different part of an existing company.\nTogether with Mr. Kirenz we decided to first define our use case to then create a business model from there on. Our use case was based on the idea of finding a way to make business analysis quick and easy for a wide range of potential customers. After that we constructed the business model around this use case. You can read more about the use case identification and modelling of our Business Model Canvas here. For a more visual understanding of our startups’ next goals we created a strategy map showing the directions our start up would go in the future.\nOur solution takes the findings of our use case identification and transform them into a software as a service (SaaS) solution. This solution provides an analysis tool for our customers to fulfill our value propositions.\nWe are using a Retrieval Augmented Generation (RAG) pipeline to load and retrieve the needed information (coming from business reports) from our vector store. We encountered different obstacles on our way, therefore we decided to customize the basic RAG pipline to deal with image/ table recognition and at the same time improve answer precision by using a multi query approach.\nWe used multiple evaluation methods to evaluate the performance of our customized RAG. The main goal was here to identify the overall performance of our custom RAG compared to the basic RAG. At the same time we evaluated the best chunk size for both RAGs. The best solution was our custom RAG, using the chunk size of 900.\nWe also developed a user friendly frontend to enable our customers to navigate between their business reports, analyses, and comparisons. Focus here was especially on the presentation of the extensive information in the SWOT analyses.\nFinally we created a comprehensive manual on how to deploy our application and how to use our application.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Project Overview</span>"
    ]
  },
  {
    "objectID": "usecase.html",
    "href": "usecase.html",
    "title": "Usecase",
    "section": "",
    "text": "We want to create a SaaS solution that is aimed at business strategy analysts of a company (see personas) to support them in tasks such as competitor analysis.\nIn our tool, the user should be able to:\n\nupload their own annual report, use an existing one or a public one to gain insights into the respective company\ncreate a SWOT analysis for the selected report\nask individual questions about the report via a chatbot",
    "crumbs": [
      "Business Relevance",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Usecase</span>"
    ]
  },
  {
    "objectID": "userPersona.html",
    "href": "userPersona.html",
    "title": "User Persona",
    "section": "",
    "text": "Demographic Data:\nName: Alex Schmidt\nAge: 35 years\nGender: Male\nPlace of residence: Stuttgart, Germany\nFamily status: Married without kids\nInterests: Cars, technology, business news, hiking, AI\nOccupation: Strategy analyst in the department for market and competition analysis at Mercedes Benz",
    "crumbs": [
      "Business Relevance",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>User Persona</span>"
    ]
  },
  {
    "objectID": "userPersona.html#tasks-responsibilities",
    "href": "userPersona.html#tasks-responsibilities",
    "title": "User Persona",
    "section": "Tasks & Responsibilities:",
    "text": "Tasks & Responsibilities:\n\nCollect and analyze market data to identify trends and developments in the automotive industry\nAnalyze competitor data to identify strengths, weaknesses, opportunities and threats\nCreate comprehensive reports and presentations for executives to enable informed strategic decisions\nCollaborate with other departments to gather data and gain insights into various aspects of the market & the firm\nDevelop long-term strategic plans to strengthen the competitive position of Mercedes-Benz",
    "crumbs": [
      "Business Relevance",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>User Persona</span>"
    ]
  },
  {
    "objectID": "userPersona.html#pains-problems",
    "href": "userPersona.html#pains-problems",
    "title": "User Persona",
    "section": "Pains & Problems:",
    "text": "Pains & Problems:\n\nTime-consuming data collection: Manual collection of market data & competitive information takes up a significant amount of working time\nRisk of information overload: The amount of data available carries the risk that Alex could overlook important trends and patterns\nDelays in reaction time: The time-consuming manual analysis leads to delays in reacting to current market developments and competitive changes\nLimited resources for in-depth analysis: Due to the time required, Alex cannot always perform the in-depth analysis required for comprehensive strategic planning\nLack of precision in data interpretation: Manual processing of data carries the risk of misinterpretation or human error, which could affect the accuracy and reliability of his analyses",
    "crumbs": [
      "Business Relevance",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>User Persona</span>"
    ]
  },
  {
    "objectID": "bmc.html",
    "href": "bmc.html",
    "title": "Business Model Canvas and Use Case Identification",
    "section": "",
    "text": "Value Proposition\nIt is our main goal to provide easy and fast access to branch insights. Being more precise, we aim to give our customers a wide overview of the market and their (direct) competitors. To achieve this goal we offer different features in our application, including the SWOT Analysis, the BCG Matrix, a Business Model Canvas and a short profile of the relevant competitor. Our customers choose which competitors they would like to analyze. They simply select or upload the annual report of a relevant competitor and start analyzing by using our features such as the SWOT Analysis. The tool consists of many different features to analyze the annual reports. We are planning to first publish the company profile/ summary, SWOT analysis, Business Model Canvas, and BCG Matrix, to then extend these features in the future to give our customers even more options to analyze the annual reports.\nOur customers choose which competitors they would like to analyze. They simply select or upload the annual report of a relevant competitor and start analyzing by using our features such as the SWOT Analysis.\nThe tool consists of many different features to analyze the annual reports. We are planning to first publish the company profile/ summary, SWOT analysis, Business Model Canvas, and BCG Matrix, to then extend these features in the future to give our customers even more options to analyze the annual reports.",
    "crumbs": [
      "Business Relevance",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Business Model Canvas and Use Case Identification</span>"
    ]
  },
  {
    "objectID": "bmc.html#customer-segments",
    "href": "bmc.html#customer-segments",
    "title": "Business Model Canvas and Use Case Identification",
    "section": "Customer Segments",
    "text": "Customer Segments\nAs our customer Segments we decided to be focusing on companies in the automotive industry. This industry is currently extremely fast moving and the competition between those companies is on an all time high. One of the biggest reasons is the development of environmentally friendly vehicles and the rising competition by brands on the Asian market. This makes it very important for the affected companies to keep an eye on their competition and arising trends -making them the perfect customers for our analysis tool.\nMore in detail, we identified strategic-competitor analysts in the automotive industry as our end users.\nIn the future we could extend our customer base to a variety of interesting customer segments. These customers could be firms in the field of investors, finance analysts or business consulting.",
    "crumbs": [
      "Business Relevance",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Business Model Canvas and Use Case Identification</span>"
    ]
  },
  {
    "objectID": "bmc.html#customer-relationship",
    "href": "bmc.html#customer-relationship",
    "title": "Business Model Canvas and Use Case Identification",
    "section": "Customer Relationship",
    "text": "Customer Relationship\nTo build our first customer base we will step in personal contact with just a few companies in the automotive field. At the same time we are keeping this close relationship with our customers to fully understand them and their needs to optimize our products to serve them as efficiently as possible. We will also offer consulting to this customer segment to adjust or extend the features of the tool to their specific needs. Later we will make our product available to a wider range of customers and offer off the shelf software including a few basic analysis methods. This off the shelf software is the product we want to sell to a wider range of customers. To stay in touch we will set up an email newsletter to keep them posted on new feature releases and changes of our tool. This customer segment can then use our self-service to unlock these new features.",
    "crumbs": [
      "Business Relevance",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Business Model Canvas and Use Case Identification</span>"
    ]
  },
  {
    "objectID": "bmc.html#channels",
    "href": "bmc.html#channels",
    "title": "Business Model Canvas and Use Case Identification",
    "section": "Channels",
    "text": "Channels\nDepending on the product we would either choose online marketing or direct sales. In case of the “off the shelf” software with no additional features we target the broader mass; meaning we are trying to reach as many customers with the same product. For our customer specific products, tailored to a more specific need on the customer side, we approach companies in a more direct sales kind of style. We expect to make more money per sale on the customer specific software, since we price the specifications and consulting services. That allows us to use this more expensive way of customer channel.",
    "crumbs": [
      "Business Relevance",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Business Model Canvas and Use Case Identification</span>"
    ]
  },
  {
    "objectID": "bmc.html#key-partners",
    "href": "bmc.html#key-partners",
    "title": "Business Model Canvas and Use Case Identification",
    "section": "Key partners",
    "text": "Key partners\nAs our key partners we identified providers of corporate and financial data. In the beginning we will simply use the public annual reports of well known companies and let our customers upload their own data to our application. In the future we want to extend this function to also offer exclusive corporate and financial data on our application. This data will be brought in by the mentioned provider firms.\nAnother important partner is a marketing agency with experience in this field. With this partner we can get automotive companies to know us and our services.\nFacing the challenge of the groundedness and quality of the data provided by us we need auditing firms to ensure high quality of the data provided by our application.",
    "crumbs": [
      "Business Relevance",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Business Model Canvas and Use Case Identification</span>"
    ]
  },
  {
    "objectID": "bmc.html#key-activities",
    "href": "bmc.html#key-activities",
    "title": "Business Model Canvas and Use Case Identification",
    "section": "Key activities",
    "text": "Key activities\nOne of our most important activities is software development. Especially in the beginning it is key to improve and extend the features of our product. Starting with our main feature, the SWOT matrix, we need to introduce more features and more ways to analyze the competition and market. Also the maintenance and updating of the database is a key activity. Another important activity is customer support. We want to help our customers with their problems -making it easy to reach out to us, which makes it easier for us to understand our customer needs, improve the product, and finally make our customers happy.",
    "crumbs": [
      "Business Relevance",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Business Model Canvas and Use Case Identification</span>"
    ]
  },
  {
    "objectID": "bmc.html#key-resources",
    "href": "bmc.html#key-resources",
    "title": "Business Model Canvas and Use Case Identification",
    "section": "Key resources",
    "text": "Key resources\nThe most important Resource in our startup is the software developers. They ensure the quality and functionality of our product. Additional to that we need the hardware for our employees.",
    "crumbs": [
      "Business Relevance",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Business Model Canvas and Use Case Identification</span>"
    ]
  },
  {
    "objectID": "bmc.html#cost-structure",
    "href": "bmc.html#cost-structure",
    "title": "Business Model Canvas and Use Case Identification",
    "section": "Cost structure",
    "text": "Cost structure\nOur Cost structure mainly consists of fixed cost for employee salaries for product development, maintenance, and consulting services. We also need to pay licenses for the commercial use of (development) software as fixed costs. At the same time we need to attract new customers, resulting in marketing expenses. For variable costs we pay for every API calls to OpenAI, made by our customers.",
    "crumbs": [
      "Business Relevance",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Business Model Canvas and Use Case Identification</span>"
    ]
  },
  {
    "objectID": "bmc.html#revenue-streams",
    "href": "bmc.html#revenue-streams",
    "title": "Business Model Canvas and Use Case Identification",
    "section": "Revenue streams",
    "text": "Revenue streams\nFor our SaaS business model, our revenue stream is structured around several key components. Firstly, we offer licensing of our software tool, allowing customers to gain access to our platform and its basic functionalities. Secondly, we provide subscription-based access to our SaaS solution, offering different tiers or plans that unlock additional features and capabilities based on the customer’s needs and usage requirements. This subscription model ensures recurring revenue as customers continue to utilize our platform over time.\nIn addition to subscriptions, we offer maintenance and support services to ensure that our customers receive ongoing assistance, updates, and troubleshooting to keep their software running smoothly. This component not only enhances customer satisfaction but also provides an additional revenue stream through service contracts or subscription add-ons.\nMoreover, we differentiate ourselves by offering consulting services to tailor our software to the specific needs and requirements of our customers. Our team of experts works closely with clients to understand their business processes, customize the software accordingly, and provide training and guidance for optimal utilization. These consulting services serve as a valuable revenue stream, allowing us to provide personalized solutions while generating additional income beyond the software subscription fees.\nOverall, our revenue stream encompasses a combination of software licensing, subscription-based access, maintenance and support services, and consulting offerings, providing a comprehensive and sustainable approach to monetizing our SaaS business model.",
    "crumbs": [
      "Business Relevance",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Business Model Canvas and Use Case Identification</span>"
    ]
  },
  {
    "objectID": "strategyMap.html",
    "href": "strategyMap.html",
    "title": "Strategy Map",
    "section": "",
    "text": "Financial\n\n\n\n\n\nMonthly revenue\n\n\nYearly revenue\n\n\n\n\nCompany growth\n\n\n\n\n\n\nGoss profit margin\n\n\nNet margin\n\n\n\n\nProfitaility\n\n\n\n\n\n\n\nCustomer\n\n\n\n\n\nCustomer aquistion cost\n\n\nCustomer retention rate\n\n\n\n\nCustomer acquisition and customer loyalty\n\n\n\n\n\n\nUser rating\n\n\nSupport response time\n\n\n\n\nOptimized user experience\n\n\n\n\n\n\n\nInternal process\n\n\n\n\n\nFrequency of featrue releases\n\n\nMeasurement of product quality\n\n\nSpeed of data import\n\n\nQuality of the output data\n\n\n\n\nFurther development and maintenance of the software\n\n\nImproving data quality and analysis options\n\n\n\n\nPersonalized service\n\n\nOff the shelf software\n\n\n\n\nIntroducing new products\n\n\n\n\n\n\n\nLearning and growth\n\n\n\n\n\nTraining sessions\n\n\nTeam satisfaction\n\n\n\n\nFurther development of the team\n\n\n\n\n\n\nNumber of new releases\n\n\nNumber of new ideas implemented\n\n\n\n\nPromoting openness to innovation in the company",
    "crumbs": [
      "Business Relevance",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Strategy Map</span>"
    ]
  },
  {
    "objectID": "valueProposition.html",
    "href": "valueProposition.html",
    "title": "Value Proposition",
    "section": "",
    "text": "Customer Segment",
    "crumbs": [
      "Business Relevance",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Value Proposition</span>"
    ]
  },
  {
    "objectID": "valueProposition.html#customer-segment",
    "href": "valueProposition.html#customer-segment",
    "title": "Value Proposition",
    "section": "",
    "text": "Gains\n\nWell-developed strategy\nGood understanding of your own company and competitors\n\n\n\nPains\n\nTime-consuming work through annual reports\nDoes not know all the data from the own company and competitor companies\nRequires significant working time to prepare the analysis\n\n\n\nCustomer Jobs\n\nCreate analysis\nCreate evaluations\nDerive and develop strategies based on the analyses and evaluations",
    "crumbs": [
      "Business Relevance",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Value Proposition</span>"
    ]
  },
  {
    "objectID": "valueProposition.html#product-segment",
    "href": "valueProposition.html#product-segment",
    "title": "Value Proposition",
    "section": "Product Segment",
    "text": "Product Segment\n\nGain Creators\n\nA mature strategy based on data and benchmarks\nBetter understanding and classification of the company\nSaves time\nRegular and simple strategy adjustments\n\n\n\nPain relievers\n\nIndividual needs can be taken into account\nStrategy and analyses can be created simultaneously\n\n\n\nProduct and services\n\nAnalysis of your own data and the competitions\nPreparation of evaluations and analysis (e.g. SWOT)\nDevelopment of strategic approaches",
    "crumbs": [
      "Business Relevance",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Value Proposition</span>"
    ]
  },
  {
    "objectID": "RAGpipeline.html",
    "href": "RAGpipeline.html",
    "title": "RAG Pipeline",
    "section": "",
    "text": "Introduction to Retrieval Augmented Generation (RAG)\nRetrieval-Augmented Generation (RAG) represents a significant advancement in the field of natural language processing (NLP) by combining the strengths of large language models (LLMs) with specific knowledge stored in external databases. (Li et al. 2022) They are particularly useful for knowledge-intensive Natural Language Processing (NLP) tasks (Lewis et al. 2020) and thus have been applied to various tasks including dialogue response generation, machine translation, and other generation tasks. (Li et al. 2022)\nAs the focus of our project is to generate and compare SWOT Analyses, which is a complex generation task, our main goal was to implement a RAG pipeline that is capable of generating factual and relevant SWOT Analyses using the companies annual report as the knowledge source. At the beginning of the project we explored basic RAG pipelines and then started to implement our own pipeline to suit our needs.",
    "crumbs": [
      "System Design",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>RAG Pipeline</span>"
    ]
  },
  {
    "objectID": "RAGpipeline.html#rag-pipelines",
    "href": "RAGpipeline.html#rag-pipelines",
    "title": "RAG Pipeline",
    "section": "RAG Pipelines",
    "text": "RAG Pipelines\n\nLangchain Base RAG\nThe first RAG pipeline we explored was build using the open-source library Langchain. The pipeline utilized the standard Langchain document loaders, text splitters and RetrievalQA chain. The code below is an extract from our first RAG pipeline. The code for the full pipeline can be viewed in the RAG-Prototype folder of the StratMystiqPro repository.\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\n\n\nopenai.api_key = os.environ['OPENAI_API_KEY']\nembedding_function = OpenAIEmbedding\n\nclient = chromadb.HttpClient(host=\"localhost\", port='8000')\ndb = Chroma(client=client,\n            collection_name=\"annual-reports\",\n            embedding_function=embedding_function)\n\nllm_name = \"gpt-3.5-turbo\"\nllm = ChatOpenAI(model_name=llm_name, temperature)\n\ntemplate = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't make up an answer. Use five sentences maximum. Keep the answer as concise as possible.\nContext: {context}\nQuestion: {question}\nHelpful Answer:\"\"\"\n\nQA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=db.as_retriever(),\n    return_source_documents=True,\n    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n\nquestion = \"What cars does Mercedes Benz produce?\"\n\nresult = qa_chain({\"query\": question})\nWe started experimenting with the Langchain pipeline and while it returned some relevant results, we found that the pipeline struggled to return relevant and factual information when the question was more complex or asked for specific numbers. We tried to identify the cause of the issue and identified three areas that we believed could be causing the issue:\n\nThe original document was not being split into relevant sections or the sections only contained partial information.\nA lot of relevant information is stored in tables and images, which the pipeline is not able to extract.\nThe question that was asked might not include the correct vocabulary or context to retrieve the relevant chunks.\n\nTo continue with our RAG pipeline development, we decided to look at current literature and their approaches to RAG pipelines.\n\n\nCustom RAG Pipeline\nDuring our research we came across the concept of advanced RAG. Advanced RAG is an evolution of the base or Naive RAG, designed to overcome its limitations through targeted improvements. In Advanced RAG, the focus is on refining the retrieval process with pre-retrieval and post-retrieval strategies as well as improving the document indexing. (Gao et al. 2023)\nWe decided to follow the Advanced RAG approach of Gao et al. and implement our own custom RAG pipeline. The three areas that we implemented for our custom RAG pipeline are:\n\nDocument Ingestion\n\nTable Extraction and Processing\nText Chunking\n\nPre-Retrieval\n\nQuery Expansion\nQuery Reformulation\n\nPost-Retrieval\n\nDocument Filtering\nRelevance Checking\n\n\nThe following image shows how the base RAG pipeline and the custom RAG pipeline differ. The orange boxes represent the areas where we implemented our own custom solutions.\n\n\n\nRAG Pipeline | Adapted from (Gao et al. 2023)\n\n\n\nDocument Ingestion\nIn the document ingestion phase, we implemented a custom solution to extract and process the relevant information from the annual reports. We noticed that a lot of relevant information was stored in tables and images, which the base RAG pipeline was not able to extract.\n\nTable Extraction\nWe first focused our attention on extracting the relevant information from the tables in the annual reports. We initially hoped that PDF files contain metadata that would identify the tables, but that is not the case. Through our research we came across multiple python libraries that claim to be able to identify and extract tables from PDF files. We tested the libraries camelot-py, tabula-py and pyMuPDF and found that none of them were perfect at identifying and extracting tables from the annual reports. However, pyMuPDF seemed to be the most accurate and therefore we decided to use it for our table extraction process.\nThe following code shows how we used pyMuPDF to identify and extract the tables from the annual reports.\nimport fitz\nimport uuid\nimport os\n\n# Load the annual report\ndoc = fitz.open(report_filepath)\n    for page in doc:\n        table_data = page.find_tables(\n                        vertical_strategy=\"text\",\n                        horizontal_strategy=\"lines\"\n                        )\n        if len(table_data.tables) &gt; 0:\n\n            # Margin in points (2 cm)\n            margin = 56.7\n\n            # Scaling factor to increase resolution\n            scale = 2\n\n            # Applying the scaling factor\n            matrix = fitz.Matrix(scale, scale)\n\n            for table in table_data:\n                # Expanding bounding box with margin\n                expanded_bbox = (\n                    max(table.bbox[0] - margin, 0),  # left\n                    max(table.bbox[1] - margin, 0),  # top\n                    min(table.bbox[2] + margin, page.rect.width),  # right\n                    min(table.bbox[3] + margin, page.rect.height)  # bottom\n                )\n\n                # Cropping the page to the expanded bounding box\n                rect = fitz.Rect(expanded_bbox)\n                cropped_table = page.get_pixmap(matrix=matrix, clip=rect)\n\n                # Checks if correct directory exists\n                if not os.path.exists(f\"./filestore/{report_id}\"):\n                    os.makedirs(f\"./filestore/{report_id}\")\n\n                # Saving the cropped table as a JPG\n                table_id = str(uuid.uuid4())\n                cropped_table.save(f\"./filestore/{report_id}/{table_id}.jpg\")\nThe screenshots of the tables can then be used to extract the relevant information from the tables. We decided to try AWS Textract and OpenAI’s Vision API to extract the relevant information from the tables. We found that OpenAI’s Vision API was more adaptable in extracting the relevant information from the tables as we could specify the output format and the specific information we wanted to extract.\nTo use the Vision API, we also had to provide a prompt to the API that specifies the information we want to extract and the output format. This required a number of prompt iterations to get a reliable and accurate output. The initial prompt and the final prompt we used are shown below:\nInitial Prompt:\ntable_extract_prompt_template = \"\"\"Return the complete table content in JSON format. In the JSON also include a short description of the tables. Use the following as a template:\n{\"tables\": [\n   {\n        \"title\": \"Forschung und Entwicklung\",\n        \"description\": \"Forschung und Entwicklung\",\n        \"unit\": \"in Millionen €\",\n        \"table_data\": [...]\n   },\n   {...}\n   ]\n}\nDO NOT RETURN ANYTHING ELSE!\"\"\"\nFinal Prompt:\ntable_extract_prompt_template = \"\"\"Return the complete table content in JSON format, structured to accurately represent both main and sub-data entries. The JSON should include a short description of the tables. If the table includes hierarchical data, such as a total figure with underlying subdivisions, ensure to represent this relationship in the JSON structure.\n\nFor tables with year-wise data:\n- Include main data entries with their corresponding sub-data as nested entries.\n- Clearly label data for each year.\n\nFor tables without year-wise data:\n- Adjust the structure to suit the table's format, still capturing any hierarchical data relationships.\n\nIn all cases, do not calculate or infer values. Only include data that is clearly visible and legible in the table. Do not make up or estimate data.\n\nUse the following template for tables with hierarchical year-wise data:\n\n{\n  \"tables\": [\n    {\n      \"title\": \"Forschung und Entwicklung\",\n      \"description\": \"Forschung und Entwicklung\",\n      \"unit\": \"in Millionen €\",\n      \"table_data\": [\n        {\n          \"row_description\": \"Total Earnings\",\n          \"data_by_year\": {\n            \"2021\": \"total for 2021\",\n            \"2022\": \"total for 2022\"\n          },\n          \"sub_data\": [\n            {\n              \"sub_row_description\": \"Subdivision 1\",\n              \"data_by_year\": {\n                \"2021\": \"data for 2021\",\n                \"2022\": \"data for 2022\"\n              }\n            },\n            // Additional subdivisions can be added here\n          ]\n        },\n        // Additional main rows can be added here\n      ]\n    },\n    // Additional tables can be added here\n  ]\n}\n\nAdapt the structure for tables without year-wise data or hierarchical relationships as needed.\n\nDO NOT RETURN ANYTHING ELSE!\n\"\"\"\nThe whole process can also be seen in the following image:\n\n\n\nTable Extraction Process\n\n\n\n\nText Extraction\nWe didn’t implement a custom solution for text extraction as we found that the Langchain Recursive Character Text Splitter did a good job at splitting the text into chunks. We did however define a list of custom separators that ensured that the text is not split in the middle of a sentence. We declared the separators as follows:\nseparators=[\"\\n\\n\", \"(?&lt;=\\. )\"]\nThis means that when the chunk size is reached, the text will be split at either the next paragraph or the next sentence.\nWhich brings us to the question of the ideal chunk size. We couldn’t find any literature that specified the ideal chunk size for RAG pipelines, so we decided to experiment with different chunk sizes ranging from 500 to 1000 characters. The evaluation of the different chunk sizes can be found in the RAG Evaluation section.\n\n\n\nPre-Retrieval\nThe pre-retrieval phase focuses on refining the query to improve the retrieval process. The goal of refining the query is to align the semantics of the query with the semantics of the documents. (Wang, Yang, and Wei 2023) A promising concept for this is GAR (Generation Augmented Retrieval), which is described in the paper: GAR meets RAG for Zero-Shot IR. The idea is to generate additional context for the query to improve the retrieval process. This can be achieved by replacing words with better synonyms, expanding the query with related words or reformulating the query to cover slightly different aspects of the same topic. (Arora et al. 2023)\nLike Arora et al., we are utilizing OpenAI LLMs to generate additional queries for the retrieval process. We didn’t use the Rewriting Prompt that Arora et al. created, but instead used their prompt as guidance to create our own use case specific prompt. The prompt we used is shown below:\nprompt_template = \"You are a professional annual business report writer. Analyse the users question and return three questions that are similar but use business vocabulary that can be found in annual business reports. Also think about being more specific or broader in the questions. Return them in JSON format.\"\nThe following example shows the input question and the generated questions that will be used to retrieve additional context.\n# Users question\nquestion = \"What cars does Mercedes Benz produce?\"\n\n# Generate additional questions\nadditional_questions = {\n    \"q1\": \"What competitive advantages do Mercedes-Benz's product offerings possess in the luxury automotive segment?\",\n    \"q2\": \"How do the core competencies of Mercedes-Benz contribute to its market positioning within the automotive industry?\",\n    \"q3\": \"What are the key product features that differentiate Mercedes-Benz in the premium vehicle category?\"\n}\n\n\nPost-Retrieval\nThe post-retrieval phase focuses on filtering and evaluating the retrieved documents to ensure that only relevant information is passed to the LLM to generate the final answer. Both Gao et al. and Arora et al. proposed a similar approach to the post-retrieval phase. They suggest that the documents should be filtered based on their relevance then re-ranked and finally summarized/compressed. (Gao et al. 2023) (Arora et al. 2023)\nWe evaluated the three proposed steps and noticed that another step is required to ensure that the documents are relevant. When querying for chunks with different but similar/related questions, the list of all retrieved chunks ended up containing several duplicates. As this would lead to unnecessary computation and cost, we decided that the first step of the post-retrieval phase should be to filter out the duplicates.\nAfter that, we implemented the relevance check for the chunks. Arora et al. did publish their relevance check prompt, however it turned out to be impractical for our use case. Their prompt was designed to evaluate all retrieved chunks at once, ranking them with a score from 1 to 5. This proofed to be impractical for us for three reasons:\n\nThe number of retrieved chunks together with their length maxed out the input limit of the GPT-4-Turbo model. This resulted in the model not being able to evaluate all chunks at once. Meaning that the documents couldn’t be scored based on all other chunks.\nThe complex query resulted in a longer processing times, adding significant and unpredictable delays to the RAG pipeline.\nThe very high token usage for the evaluation prompt resulted in a high cost for the evaluation process.\n\nBased on these reasons, we decided to create our own relevance check prompt that evaluates each chunk individually. Our prompt is designed to evaluate the relevance of the chunk based on the original question and the chunk itself. The prompt is shown below:\nprompt_template = \"You are a professional business analyst. Analyse the given document and see if it contains information that is useful for answering the users question. Be very strict!. ONLY REPLY WITH YES OR NO. No need to be polite.\"\nBased on the relevance check, we then removed all chunks that were not relevant. Leaving us with a list of relevant chunks.\nThe next step in the model that Gao et al. suggested was to re-rank the documents. We decided to skip this step as the chunks were already ranked by the Max-Marginal-Relevance (MMR) algorithm that was used to retrieve the chunks and this additional step would once again be very token intensive. The MMR algorithm ranks the chunks based on their relevance to the query and their similarity to the other chunks. As we append the chunks found for the additional queries to the chunks found for the original query, we ensure that according to the MMR algorithm the most relevant chunks for the original question are at the top of the list.\nThe final step in the post-retrieval phase is to summarize/compress the chunks. While we would have liked to implement this step, the approaches named by Gao et al. were all based on custom trained information extraction and compression models that we didn’t have access to. To avoid more cost by having the LLM summarize the chunks, we decided to leave the chunks as they are and let the LLM generate the final answer based on the list of relevant chunks.\n\n\n\n\nArora, Daman, Anush Kini, Sayak Ray Chowdhury, Nagarajan Natarajan, Gaurav Sinha, and Amit Sharma. 2023. “Gar-Meets-Rag Paradigm for Zero-Shot Information Retrieval.” https://doi.org/10.48550/ARXIV.2310.20158.\n\n\nGao, Yunfan, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, et al. 2023. “Retrieval-Augmented Generation for Large Language Models: A Survey.” https://doi.org/10.48550/ARXIV.2312.10997.\n\n\nLewis, Patrick, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, et al. 2020. “Retrieval-Augmented Generation for Knowledge-Intensive Nlp Tasks.” https://doi.org/10.48550/ARXIV.2005.11401.\n\n\nLi, Huayang, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022. “A Survey on Retrieval-Augmented Text Generation.” https://doi.org/10.48550/ARXIV.2202.01110.\n\n\nWang, Liang, Nan Yang, and Furu Wei. 2023. “Query2doc: Query Expansion with Large Language Models.” https://doi.org/10.48550/ARXIV.2303.07678.",
    "crumbs": [
      "System Design",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>RAG Pipeline</span>"
    ]
  },
  {
    "objectID": "RAGevaluation.html",
    "href": "RAGevaluation.html",
    "title": "RAG Evaluation",
    "section": "",
    "text": "Fundamental Goals of Evaluation\n\n\nMethods for RAG-Evaluation\nThere are different options when it comes to evaluating a RAG-System. After looking into the literature we found different out that there is not the one perfect method to evaluate a RAG-System.\nIn Information Retrieval (IR) Venable et al. (2016) looks at the whole development process of an IR System. In the following Figure XX we see different approaches for different products. Our product falls in the “Human Risk & Effectiveness” path. This path is made for solutions that are human centered just as our system. The black triangles are symbolizing different possible evaluation steps. On the y axis we find a scale coming from artificial, going to more naturalistic. Artificial stands for a more automated evaluation process. On the other hand we find a scale from formative to summative on the x axis. Formative means that with this evaluation we try to improve the output; while summative evaluations should give an more generall overview over the final product and whether it is providing all the functionalities it should provide as a solution a customer could use.\nSince our product is still in the prototype phase we concentrated our evaluation on the first three triangles. Meaning we did an automated evaluation using libraries like TruLens and RAGAS. The biggest advantage coming with these techniques is that they are comparably cheap (Venable et al., 2016). Next we used human evaluation to prove the results of our automated evaluation approach with this human centered approach. Human centered approaches for evaluation bring the advantage to give a more realistic view on the system. Problem with these approaches is that they are comparably expensive and time consuming (Venable et al., 2016)\n\n\nGeneral Evaluation Foundation\nWe used our evaluation methods always on both, the basic RAG and our customized RAG. To do so we handed over the following eight questions. On the one hand question 1,2,3,5 are more “simple” questions, aiming to find a simple answer. Question 4,6,7,8 on the other hand are more “complex” questions, aiming to find a solution, where multiple contexts are needed from the business report.\nPrompts to RAG in evaluation process:\n\nWhat product categories and car models does Mercedes-Benz offer?\nWhat was the total revenue of Mercedes-Benz in the last fiscal year?\nWhich new markets did Mercedes-Benz enter in the past year?\nWhat specific sustainability goals has Mercedes-Benz set for the upcoming year?\nHow much did Mercedes-Benz invest in research and development in the last fiscal year?\nWhat are the key challenges Mercedes-Benz has faced in its supply chain and production processes, and how have these impacted its operational efficiency?\nHow is Mercedes-Benz adapting its overall business strategy to align with current global automotive trends, such as digitalization, sustainability, and changing consumer preferences?\nWhat opportunities does Mercedes-Benz have in the realm of electric vehicles and autonomous driving technologies, and what strategic initiatives are they undertaking in these areas?\n\n\n\nAutomated Evaluation Approach\nAs already mentioned we used different libraries to evaluate our RAG-System.\nFinally we ended up using the RAGAS framework since this made it easy to extract the evaluation results in an excel file to later compare it with our human centered evaluation approach. Following you we see the results of our evaluation using the RAGAS framework.\n\n\nHuman Centered Evaluation Approach\n\nThree evaluators\nEvaluating the RAG-results based on the three criteria: “Relevance and Accuracy”, “Clarity and Coherence”, and “Completeness and Depth”\nPresentation of results…",
    "crumbs": [
      "System Design",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>RAG Evaluation</span>"
    ]
  },
  {
    "objectID": "backendArchitecture.html",
    "href": "backendArchitecture.html",
    "title": "Backend Architecture",
    "section": "",
    "text": "Backend Requirements\nAt the beginning of the project, we were trying to decide what technologies and architectures we should use to build our system. To make these decisions, we decided to to define the requirements that the system should meet from a technical standpoint. Seeing that the goal of this project is to build a prototype for a SaaS tool, the backend should be designed to meet a number of requirements. These have been grouped into the following categories:",
    "crumbs": [
      "System Design",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Backend Architecture</span>"
    ]
  },
  {
    "objectID": "backendArchitecture.html#backend-requirements",
    "href": "backendArchitecture.html#backend-requirements",
    "title": "Backend Architecture",
    "section": "",
    "text": "Scalable\nThe system should be designed to handle multiple number of users and tasks at any time without impacting performance.\n\nHandle multiple different Users at any time:\n\nProcess a variety of reports simultaneously.\nProvide answers to user queries efficiently.\n\nAllow easy horizontal or vertical scalability:\n\nThe system should be capable of scaling out (adding more nodes to the system) and scaling up (adding more power to existing nodes) as necessary, without any service interruption.\n\n\n\n\nResource Efficient\nOptimizing the use of resources is crucial for maintaining a cost-effective and high-performing system.\n\nMinimize API-Calls to OpenAI and other external services:\n\nReduce the frequency and volume of API requests to avoid unnecessary costs and to maintain system responsiveness.\n\nOnly regenerate responses if requested by a User:\n\nCache and reuse responses where possible to avoid redundant computations.\n\n\n\n\nAdaptable\nThe system should be flexible to accommodate changes and the addition of new features.\n\nAllow Users to upload reports from several business sectors:\n\nSupport a diverse range of report formats and contents to cater to different industry needs.\n\nIt should be easy to add features to the tool without major code changes:\n\nDesign the architecture to be modular, allowing new features to be added with minimal changes to the core codebase.\n\n\n\n\nSecure\nSecurity is a top priority to protect user data and ensure trust in the system.\n\nUser Log-In should be secure and follow industry standards:\n\nImplement authentication mechanisms that meet current security best practices.\n\nEnsure User Data is separated and protected from unauthorized access:\n\nApply strict access controls and data isolation to ensure that user data is not exposed to unauthorized entities.",
    "crumbs": [
      "System Design",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Backend Architecture</span>"
    ]
  },
  {
    "objectID": "backendArchitecture.html#tech-stack",
    "href": "backendArchitecture.html#tech-stack",
    "title": "Backend Architecture",
    "section": "Tech Stack",
    "text": "Tech Stack\nBased on the requirements defined above, we decided to use the following technologies to build the backend of the system. The following sections will provide a brief overview of each technology and the reasons for choosing them.\n\nPython\nThe backend of the system is built using Python, some of the reasons for choosing Python are:\n\nEase of Use:\n\nAll team members have experience working with Python, making it easy for all team members to actively contribute to the project.\n\nNumerous Opensource Libraries:\n\nPython has a rich ecosystem of libraries and tools ranging from backend frameworks to machine learning libraries such as Flask and Langchain.\n\n\n\n\nFastAPI\nFastAPI is a modern, fast (high-performance), web framework for building APIs with Python. The reasons for choosing FastAPI are:\n\nPerformance and Asynchronous Request Handling:\n\nFastAPI is built on top of Starlette for web routing and Pydantic for data validation, this makes it more performant than other Python web frameworks such as Flask.\nIt supports asynchronous request handling natively, which is crucial for handling multiple requests simultaneously where long-running tasks are expected.\n\nAutomatic API Documentation:\n\nFastAPI automatically generates API documentation based on the code, which makes it easier to maintain and update the documentation.\nIt provides a built-in interactive API documentation (Swagger UI) which makes it easier to test and debug the API. Without the need for additional tools such as Postman.\n\n\n\n\nDocker\nDocker is a platform for developing, shipping, and running applications. The reasons for choosing Docker are:\n\nConsistent Development Environment:\n\nDocker allows us to create a consistent environment for development as it greatly reduces the number of possible errors when working with different platforms (Mac and Windows).\nDatabases and other services can be run as containers, which makes it easier to set up and tear down the development environment quickly.\n\nEasy Deployment:\n\nDocker containers can be deployed to any platform that supports Docker, which makes it easier to deploy the system to different cloud providers.\nServices can be scaled horizontally by running multiple containers of the same service.\n\n\n\n\nMySQL\nTo store user, report and analysis data, we decided to use a relational database. The reasons for choosing MySQL are:\n\nExperience:\n\nAll team members have experience working with MySQL, making it easier for all team members to actively contribute to the project.\n\nScalability:\n\nShould the need arise, MySQL can be scaled horizontally by using sharding or clustering to handle a large number of users and reports.\n\n\n\n\nChromaDB\nChromaDB is an open-source vector store that is designed to store and retrieve vector embeddings. The reasons for choosing ChromaDB are:\n\nNatively Supported by Langchain:\n\nChromaDB is natively supported by Langchain modules and thus requires minimal configuration to build initial prototypes.\n\nDocumentation and Community:\n\nChromaDB has a rich documentation and an active community which makes it easier to get help and support when needed.",
    "crumbs": [
      "System Design",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Backend Architecture</span>"
    ]
  },
  {
    "objectID": "backendArchitecture.html#data-model",
    "href": "backendArchitecture.html#data-model",
    "title": "Backend Architecture",
    "section": "Data Model",
    "text": "Data Model\nThe data model of the application is designed to store user data, report data, and analysis data. The following sections will provide a brief overview of the data model and the relationships between the different entities. The data model is designed to be flexible and adaptable to accommodate changes and the addition of new features.\n\nOverview Diagram\nThe following diagram provides an overview of the data model and the relationships between the different entities. As well as the relationship between the relational database and vector store.\n\n\n\nData Model of StratMystiqPro\n\n\n\n\nUser\nThe User entity represents a user of the system. It is created when a user logs in to the system for the first time. The User entity has the following attributes:\n\nuser_id: (PK:UUID) A unique identifier for the user.\nusername: (String) The username of the user.\nemail: (String) The email address of the user.\nrole: (String) The role of the user, which can be either “admin” or “user”.\ncreated_at: (DateTime) The date and time when the user was created.\nupdated_at: (DateTime) The date and time when the user was last updated.\n\n\n\nReport\nThe Report entity is the central entity of the system. It represents a report that a user uploads to the system. A Report by default is private and can only be accessed by the user who uploaded it. The User does have the option to make the report public. The Report entity has the following attributes:\n\nreport_id: (PK:UUID) A unique identifier for the report.\ntitle: (String) The title of the report.\nfilename: (String) The filename of the report.\nyear: (Integer) The year the report was published.\ncompany_id: (FK:UUID) The company identifier of the report. Foreign key to the Company entity.\nreport_public: (Boolean) A flag to indicate if the report is public or private.\ncreated_at: (DateTime) The date and time when the report was created.\nupdated_at: (DateTime) The date and time when the report was last updated.\ndeleted_at: (DateTime) The date and time when the report was deleted.\n\n\n\nCompany\nThe Company entity represents the company that the report is associated with. The Company entity has the following attributes:\n\ncompany_id: (PK:UUID) A unique identifier for the company.\nname: (String) The name of the company.\nindustry: (String) The industry of the company.\ncreated_at: (DateTime) The date and time when the company was created.\nupdated_at: (DateTime) The date and time when the company was last updated.\n\n\n\nAnalysis\nThe Analysis entity is the object that contains the results of the main feature of the system. It represents the analysis of a report that a user requests. Note that the Analysis entity is specifically not called SWOT Analysis, as the system is designed to be flexible and adaptable to accommodate new types of analysis in the future. The Analysis entity has the following attributes:\n\nanalysis_id: (PK:UUID) A unique identifier for the analysis.\nreport_id: (FK:UUID) Reference to the report that the analysis is associated with.\ncompare_report_id: (FK:UUID) If the analysis is a comparison analysis, this field will reference the report that was used for comparison.\ntype: (String) The type of analysis, currently supported are “swot” or “swot_comparison”.\ncontent: (Text) The content of the analysis, which is a JSON string that contains the results of the analysis.\ncreated_at: (DateTime) The date and time when the analysis was created.\nupdated_at: (DateTime) The date and time when the analysis was last updated.\n\n\n\nAnalysis Section\nThe Analysis Section entity represents a section of the analysis. It was introduced towards the end of the project to store intermediate results of the analysis. In case an analysis process is interrupted, the system can use the intermediate results to continue the analysis where it stopped. The Analysis Section entity has the following attributes:\n\npartial_result_id: (PK:UUID) A unique identifier for the partial result.\nanalysis_id: (FK:UUID) Reference to the analysis that the partial result is associated with.\nsection: (String) The section of the analysis, which can be “strengths”, “weaknesses”, “opportunities” or “threats”.\ncontent: (Text) The content of the partial result, which is a JSON string that contains the intermediate results of the analysis.\ncreated_at: (DateTime) The date and time when the partial result was created.\nupdated_at: (DateTime) The date and time when the partial result was last updated.\n\n\n\nScreenshot\nThe Screenshot entity is used to track and identify the screenshots that are taken of the reports. The Screenshots are either the identified tables or the cover page of the report. It is implemented in a way that allows all other types of screenshots to be added in the future. The Screenshot entity has the following attributes:\n\nscreenshot_id: (PK:UUID) A unique identifier for the screenshot.\nreport_id: (FK:UUID) Reference to the report that the screenshot is associated with.\nfilename: (String) The filename of the screenshot.\ntype: (String) The type of the screenshot, which can be “table” or “image”.\ndescription: (Text) A JSON string that contains the information extracted from the screenshot.\ncreated_at: (DateTime) The date and time when the screenshot was created.\nupdated_at: (DateTime) The date and time when the screenshot was last updated.\n\n\n\nJob\nThe Job entity was originally introduced to track the status of the report ingestion. While developing the system, we realized that we need to track the status of many long-running task in the system. It also acts a Job-List for the Backend to process outstanding jobs. The Job entity has the following attributes:\n\njob_id: (PK:UUID) A unique identifier for the job.\nobject_id: (UUID) Reference to the object that the job is associated with. Currently supported are report_id, analysis_id or screenshot_id.\nobject_type: (String) The type of the object that the job is associated with. Currently supported are “report”, “analysis” or “screenshot”.\nuser_id: (FK:UUID) Reference to the user that the job is associated with.\nstatus: (String) The status of the job, which can be “pending”, “in_progress”, “completed” or “failed”.\npercentage: (Integer) The percentage of the job that has been completed. (Only supported in report_ingestion)\nrequest_date: (DateTime) The date and time when the job was requested.\ncompletion_date: (DateTime) The date and time when the job was completed.\ncreated_at: (DateTime) The date and time when the job was created.\nupdated_at: (DateTime) The date and time when the job was last updated.\n\n\n\nVector Store\nIn the report ingestion process, the system extracts the text from the report, splits it into chunks and stores it in the vector store. The vector store is used to store the vector embeddings of the text, the text that was embedded, as well as metadata to identify the individual chunks. The Vector Store knows the following two entities:\n\nCollection\nThe Collection entity represents a group of vector embeddings. We currently only use one collection to store the embeddings of all reports. For future use cases, we planned on adding more collections to store embeddings of different types of data (Websites…) The Collection entity has the following attributes:\n\ncollection_id: (PK:UUID) A unique identifier for the collection.\nname: (String) The name of the collection.\ncreated_at: (DateTime) The date and time when the collection was created.\n\n\n\nChunk\nThe Chunk entity represents a single chunk of text that was embedded and stored in the vector store. The Chunk entity has the following attributes:\n\nchunk_id: (PK:UUID) A unique identifier for the chunk.\nvectors: (Array) An array of floats that represents the vector embeddings of the text.\ntext: (Text) The text that was embedded.\nmetadata: (JSON) A JSON string that contains metadata to identify the chunk.\n\nreport_id: (UUID) The report the chunk is associated with. Links to the Report entity in the relational database.\nchunk_type: (String) The type of the chunk, which can be “table” or “report_text”.\nextra_content: (JSON) Only used for “table” chunks. Contains the extracted table data.\npage_number: (Integer) The page number of the report where the chunk was extracted from.",
    "crumbs": [
      "System Design",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Backend Architecture</span>"
    ]
  },
  {
    "objectID": "backendArchitecture.html#python-classes",
    "href": "backendArchitecture.html#python-classes",
    "title": "Backend Architecture",
    "section": "Python Classes",
    "text": "Python Classes\nThe following section will focus on the Python classes that were implemented to handle the business logic of the system. It should be noted that this structure was not implemented from the beginning of the project, but has evolved over time as the system grew in complexity. Specifically, the classes that represent the entities of the system such as User, Report, Analysis, etc. don’t contain any business logic and are only used to represent the data model of the system. They are needed to interact with the relational database using the SQLAlchemy ORM.\nFurthermore, certain areas of the system such as the implementation of the FastAPI endpoints are not programmed in a class-based structure, but rather in a function-based structure as recommended by the FastAPI documentation. The following sections will provide a brief overview of the relevant classes and their responsibilities.\n\nClass Diagram\nThe following diagram provides an overview of the classes and their relationships. The diagram does not contain the FastAPI endpoints, and authentication logic as they are not implemented as classes. \n\n\nConfig\nThe Config class is responsible for loading environment variables, initializing the OpenAI API, the Chroma Vector Store, and the MySQL Database. It does this in its init method. The environment variables are loaded from a .env file or from the Docker environment, depending on the environment in which the application is running.\nThe Config class is used to manage the configuration of the application. It provides a centralized place to manage all configuration settings. This makes it easier to change settings and manage dependencies as they are all in one place.\nThe Config class is written as a Singleton class. The Singleton pattern is a design pattern that restricts the instantiation of a class to a single instance. This is important as having multiple instances of the Config class could lead to inconsistencies in the configuration settings across different parts of the application. Furthermore, the Config class is responsible for initializing several dependencies such as the database connections. These resources should be shared across the application and not duplicated.\n\n\nDatabase\n\n\nChromaConnector\n\n\nBusinessAnalyst",
    "crumbs": [
      "System Design",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Backend Architecture</span>"
    ]
  },
  {
    "objectID": "backendArchitecture.html#authentication",
    "href": "backendArchitecture.html#authentication",
    "title": "Backend Architecture",
    "section": "Authentication",
    "text": "Authentication",
    "crumbs": [
      "System Design",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Backend Architecture</span>"
    ]
  },
  {
    "objectID": "backendArchitecture.html#rest-api",
    "href": "backendArchitecture.html#rest-api",
    "title": "Backend Architecture",
    "section": "REST API",
    "text": "REST API\n\n\n\nData Model of StratMystiqPro",
    "crumbs": [
      "System Design",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Backend Architecture</span>"
    ]
  },
  {
    "objectID": "BackendArchitecture/GoogleAuth.html",
    "href": "BackendArchitecture/GoogleAuth.html",
    "title": "Authentication with Google",
    "section": "",
    "text": "Process flow of the authentication\nThis user object now contains all the important information that is displayed on the start page and in the navigation bar, among other things, and the token that is sent with every backend request. On the backend side, we force the JWT token to be sent in the header when an endpoint is called, otherwise it is not even possible to execute the called function.",
    "crumbs": [
      "System Design",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Authentication with Google</span>"
    ]
  },
  {
    "objectID": "BackendArchitecture/GoogleAuth.html#process-flow-of-the-authentication",
    "href": "BackendArchitecture/GoogleAuth.html#process-flow-of-the-authentication",
    "title": "Authentication with Google",
    "section": "",
    "text": "User logs in by clicking the button\nGoogle returns JWT\nSends JWT to the backend\nBackend checks whether the user exists in our DB in the “user” table\nIf not, a user is created in the DB and the user object is sent to the frontend, if yes, this user object is sent to the frontend\nIn the frontend, a separate user object is created in the session memory, which contains both the data sent by the DB (userName and userId) and data directly from the JWT (firstName, lastName, profilePictureUrl), as well as the JWT itself. The data that we extract directly from the JWT in the frontend is not stored in the DB, which is why it is not contained in the user object that comes from the DB.",
    "crumbs": [
      "System Design",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Authentication with Google</span>"
    ]
  },
  {
    "objectID": "BackendArchitecture/GoogleAuth.html#frontend-fetching-function",
    "href": "BackendArchitecture/GoogleAuth.html#frontend-fetching-function",
    "title": "Authentication with Google",
    "section": "Frontend fetching function",
    "text": "Frontend fetching function\nFor this reason, we have created a function in the frontend that writes the JWT in the HTML header of each request. This function is used for all of our backend requests and therefore also offers additional parameters so that it can be used flexibly for any type of request. These include the mandatory specification of the CRUD method (Create, Read, Update, Delete) and the optional specification of a body. However, the latter is only used when uploading the annual report. In the event that the backend returns an error code or nothing at all, we have error handling in the frontend. Sometimes the error is only displayed on the browser console, but sometimes it is also displayed in the frontend, e.g. if you try to load a non-existent analysis. In any case, however, an error message in the backend does not cause the frontend to crash, as we are informed by Typescript of the possibility of an empty response and accordingly functions that would have accessed data from the response are not even executed.\n\n\n\nFrontend fetching function",
    "crumbs": [
      "System Design",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Authentication with Google</span>"
    ]
  },
  {
    "objectID": "BackendArchitecture/GoogleAuth.html#backend-token-validation",
    "href": "BackendArchitecture/GoogleAuth.html#backend-token-validation",
    "title": "Authentication with Google",
    "section": "Backend token validation",
    "text": "Backend token validation\nOn the backend side, this looks like this in the API. We have given each endpoint that is to be protected the attachment user=Security(verify_token.verify_token). This ensures that a token must always be passed when the function is called via the endpoint.\n\n\n\nBackend Endpoint requires user token\n\n\nTo validate the token, it is first extracted from the header and then sent to Google via an interface for verifying so-called oauth2 tokens, which report back to us whether this token is valid or not.\n\n\n\nBackend verify user token\n\n\nAfter this, the database is checked to see whether or not the user already exists. The user object is then sent back.\n\n\n\nCheck if the user exists in DB\n\n\nThis individual access includes access to the public reports and to all private reports uploaded by the user. In addition, each user can create their own SWOT analysis for each of their reports and create a SWOT comparison between two different reports.\n\n\n\nFrontend fetching function\nBackend Endpoint requires user token\nBackend verify user token\nCheck if the user exists in DB",
    "crumbs": [
      "System Design",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Authentication with Google</span>"
    ]
  },
  {
    "objectID": "Frontend/FrontendIntro.html",
    "href": "Frontend/FrontendIntro.html",
    "title": "Frontend Documentation",
    "section": "",
    "text": "Overview of the frontend functionalities:",
    "crumbs": [
      "Frontend",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Frontend Documentation</span>"
    ]
  },
  {
    "objectID": "Frontend/FrontendIntro.html#overview-of-the-frontend-functionalities",
    "href": "Frontend/FrontendIntro.html#overview-of-the-frontend-functionalities",
    "title": "Frontend Documentation",
    "section": "",
    "text": "Login and access to backend functions:\n\nRegistered users have the opportunity to use all functions of our backend. These include uploading annual reports and interacting with the RAG pipeline.\n\nAnnual report processing:\n\nOnce an annual report has been uploaded, it is sent to our backend. There it passes through the RAG pipeline as described on the Page RAG pipeline.\nIn the second step, the processed report can be used to create a SWOT analysis in the frontend.\n\nSWOT analysis generation:\n\nUsers can generate and display a SWOT analysis in the frontend.\nThe basis for this is the processed annual report from the backend.\n\nComparison of annual reports:\n\nIn addition to the individual analysis, our system offers the option of comparing analyses of two annual reports.\nUsers can view the results of these comparisons in the front end.\n\nChatbot support:\n\nWe have implemented a chatbot that allows users to ask self-formulated questions about a selected annual report.\nThe chatbot offers an interactive way of requesting information and makes it easier to navigate the system.\n\n\n\n\n\nWelcome page of StratMystiqPro\n\n\n\n\n\nWelcome page of StratMystiqPro",
    "crumbs": [
      "Frontend",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Frontend Documentation</span>"
    ]
  },
  {
    "objectID": "Frontend/Techstack.html",
    "href": "Frontend/Techstack.html",
    "title": "Frontend Techstack",
    "section": "",
    "text": "React\nThe choice to use React as a frontend instead of other web frontends, some of which are based on Python, was made because it allows us to use and style individual components, making our UI more adaptable to the use case and, in particular, to the subsequent users described in the personas. We have also already had good experience with React in the past, which we were happy to expand on in this project. Another reason is that, compared to the other frameworks tested, Streamlib and Gradio, user authentication via Google Login was much easier to implement here.",
    "crumbs": [
      "Frontend",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Frontend Techstack</span>"
    ]
  },
  {
    "objectID": "Frontend/Techstack.html#vite",
    "href": "Frontend/Techstack.html#vite",
    "title": "Frontend Techstack",
    "section": "Vite",
    "text": "Vite\nWe also opted for Vite as a development server for React, as it offers several advantages in terms of building times and ease of development. In addition to a much faster server startup time and building time for deployment, rebuilding when changing the code during development also takes place with almost no waiting time. This becomes particularly important when the size of the project increases, but the difference is also very noticeable with a project of our size. This increase in performance is achieved by so-called native ESM (ECMAScript Module) and the Hot Module Replacement (HMR) that this enables. This means that individual parts of the application can be replaced during operation without having to reload the entire page if the code is changed and this component needs to be rebuilt. This also preserves the state of the page, which means, for example, that all entries made are not lost and it is not necessary to log back in with Google. (webpack n.d.) (Vite n.d.)",
    "crumbs": [
      "Frontend",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Frontend Techstack</span>"
    ]
  },
  {
    "objectID": "Frontend/Techstack.html#typescript",
    "href": "Frontend/Techstack.html#typescript",
    "title": "Frontend Techstack",
    "section": "TypeScript",
    "text": "TypeScript\nFor the functional development of the frontend, we chose TypeScript as our programming language instead of typical JavaScript. It enables type-safe development by enforcing types and interfaces for all variables and the associated better error analysis.",
    "crumbs": [
      "Frontend",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Frontend Techstack</span>"
    ]
  },
  {
    "objectID": "Frontend/Techstack.html#tailwindcss",
    "href": "Frontend/Techstack.html#tailwindcss",
    "title": "Frontend Techstack",
    "section": "TailwindCSS",
    "text": "TailwindCSS\nFor styling, we use the TailwindCSS framework, which combines typical CSS functions into classes that can be applied directly to the individual elements in the HTML code, similar to inline css. This makes styling easier for us and, from our point of view, makes it easier to maintain the code and keep it clearer, as the styling of the individual HTML elements is not outsourced to an external CSS file, but is implemented directly in the code where it is later used. It also makes it easy to create responsive designs.",
    "crumbs": [
      "Frontend",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Frontend Techstack</span>"
    ]
  },
  {
    "objectID": "Frontend/Techstack.html#materialui",
    "href": "Frontend/Techstack.html#materialui",
    "title": "Frontend Techstack",
    "section": "MaterialUI",
    "text": "MaterialUI\nNevertheless, we used the MaterialUI component library for some components, as we did not have to create the elements ourselves. We used the framework of the dialog and dropdown component from MaterialUI for our popup and added our own content there. The advantage of this ready-made component is that it already has an animation for opening and closing, as well as a responsive design. Building this component ourselves would have taken some time and the result would have been about the same.",
    "crumbs": [
      "Frontend",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Frontend Techstack</span>"
    ]
  },
  {
    "objectID": "Frontend/Techstack.html#eslint-and-prettier",
    "href": "Frontend/Techstack.html#eslint-and-prettier",
    "title": "Frontend Techstack",
    "section": "ESlint and Prettier",
    "text": "ESlint and Prettier\nWe also use the formatting and linting tools Prettier and ESLint to increase code quality. Prettier ensures that the code is in a uniform format. This means, for example, that all indentations, breaks and spaces in brackets are always consistent across all files. ESlint helps to analyze the code for errors and avoid bad coding style. For example, it throws a warning if the same code is found in multiple files and then suggests exporting it or warns if an attempt is made to access variables or functions that do not exist. Without this help, such an error would only become apparent at runtime and possibly cause the program to crash.",
    "crumbs": [
      "Frontend",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Frontend Techstack</span>"
    ]
  },
  {
    "objectID": "Frontend/Techstack.html#google-single-sign-on",
    "href": "Frontend/Techstack.html#google-single-sign-on",
    "title": "Frontend Techstack",
    "section": "Google Single-Sign-on",
    "text": "Google Single-Sign-on\nAnother important library that we use in the frontend is that of the Google Single-Sign-on identity provider. Here we have the option of simple and reliable user authentication by requiring users to log in to our frontend with their Google account. We then identify the users via the JWT token returned by the identity provider by reading the ID it contains and comparing it with our database.\n\n\n\n\nVite. n.d. “Vite Features.” Accessed January 10, 2024. https://vitejs.dev/guide/features.html.\n\n\nwebpack. n.d. “Hot Module Replacement.” Accessed January 10, 2024. https://webpack.js.org/guides/hot-module-replacement/.",
    "crumbs": [
      "Frontend",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Frontend Techstack</span>"
    ]
  },
  {
    "objectID": "Frontend/ux.html",
    "href": "Frontend/ux.html",
    "title": "User experience",
    "section": "",
    "text": "We have developed a logical user guidance for our frontend, in which the user is guided step by step through the process from uploading the PDF with the annual report to creating and viewing the finished SWOT analysis. (More in the User Guide)\nOverall, we have taken a minimalist approach to the design of the frontend. This means that we have made sure that we only have the most important elements on one page and arrange the individual elements in a recurring layout. Accordingly, the content of our pages is always centered and all text is left-aligned. For orientation, the navigation bar is always displayed at the top of the screen with the currently selected page highlighted. We also use only a few colors and a few gradations of them. Specifically, this is a dark blue as the background color, a lighter blue as the primary color, as well as gradations of it for all our elements and orange as an accent color, which we use for the buttons. We decided to use a dark background and a small number of colors to create a coherent and harmonious look and not distract the user from the content. We created this color combination using an online tool for creating color parallels and then adjusted it slightly. In addition, for reasons of readability and aesthetics, we use a shade of white as the text color to avoid too much contrast between the background and the text, which could otherwise lead to visual fatigue. (Albert 2016, pg. 60)\nWe have also deliberately decided to use rounded corners everywhere to create a softer look. In addition, all clickable elements are provided with a zoom effect or have a highlighted background as soon as you hover over them with the mouse so that they are even easier to recognize as such.\n\n\n\nFrontend Hover Text\n\n\n\n\n\nFrontend Hover Button\n\n\nFor reasons of accessibility, we have also made sure that the onClick functions are not placed on elements other than buttons, which also allows people with operating aids, such as a screen reader, to localize clickable elements.\nTo make it easier for users to use our tool and avoid unnecessary click paths, we automatically load the most recent analysis created by the user on the SWOT page when a report is selected, if one already exists. If no analysis has yet been created, we display a corresponding message at the top right of the screen to inform users that they must first create one. We also display such messages if the login has failed and if a new analysis is currently being generated, as this process takes some time.\n\n\n\nNo SWOT Analysis message\n\n\nDuring the upload and processing of a report, the progress is indicated by a loading bar. While the processing is running, the upload button is grayed out so that another report cannot be uploaded in the meantime.\n\n\n\nFrontend loading bar\n\n\n\n\n\nFrontend Hover Text\nFrontend Hover Button\nNo SWOT Analysis message\nFrontend loading bar\n\n\n\nAlbert, M. 2016. Besseres Mobile-App-Design: Optimale Usability Für iOS Und Android. entwickler.Press.",
    "crumbs": [
      "Frontend",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>User experience</span>"
    ]
  },
  {
    "objectID": "Frontend/ComponentDesc.html",
    "href": "Frontend/ComponentDesc.html",
    "title": "Component description",
    "section": "",
    "text": "As is typical for React, we have built components for all our elements that we can reuse modularly elsewhere. We first planned and designed these components in Figma and then implemented them. One example of this is the complete SWOT matrix, which is used on both the SWOT page and the comparison page. The same applies to the button component, which we use wherever the orange button is required. The two tables on the My Reports page are also based on the same component. This division of the individual elements into components makes it easier for us to assemble new elements from them without having to rebuild the respective components each time. All of them also have interfaces for adjusting individual parameters, such as color, text or, of course, if they display dynamic data from the database, a way to read it in. From a development point of view, a granular division into components makes sense above all if they can be reused in other elements and therefore do not have to be developed anew for each element. Above all, this saves time and also avoids malfunctions, as you don’t have to write functionally identical or similar code multiple times. You also don’t have to make sure that you implement the same design every time, as this is already specified by the component.\nWhen styling the components, we consistently developed and implemented a design adapted for smaller screens in addition to the normal design for large screens. The frontend was developed desktop first, i.e. we focused primarily on large screen sizes and only adapted the existing design for small screens in a second step. By individually adapting the components to the mobile design only when the available space makes it necessary, we have a seamless responsiveness that works on practically all common screen sizes.",
    "crumbs": [
      "Frontend",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Component description</span>"
    ]
  },
  {
    "objectID": "deployment.html",
    "href": "deployment.html",
    "title": "Deployment",
    "section": "",
    "text": "Set up the backend for local development:\nEnsure you have Python 3.11 or later installed. * Install the required dependencies with pip install -r requirements.txt. * Ensure that you have Docker installed. Start it with using Docker Compose up * Ensure the MySQL and Chroma databases are running. These can be started using Docker Compose with the command docker-compose up mysql chromadb. * Run the application with python main.py. * The main.py script takes care of setting up the application. It creates the database tables if they don’t exist.",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Deployment</span>"
    ]
  },
  {
    "objectID": "deployment.html#setup-for-the-frontend-for-local-development",
    "href": "deployment.html#setup-for-the-frontend-for-local-development",
    "title": "Deployment",
    "section": "SetUp for the frontend for local development:",
    "text": "SetUp for the frontend for local development:\n\nEnsure that you have Node.js and npm installed\nInstall the required dependencies with: npm install\nTo run the frontend you have to use npm run dev\nDuring the developmentnpm run tailwind needs to be started as well",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Deployment</span>"
    ]
  },
  {
    "objectID": "deployment.html#to-deploy-the-application",
    "href": "deployment.html#to-deploy-the-application",
    "title": "Deployment",
    "section": "To deploy the application:",
    "text": "To deploy the application:\n\nCheck the Dockerfile and docker-compose.yaml files in the root directory of the project\nIf necessary, adjust the configuration in the Dockerfile and docker-compose.yaml files\nTo build and start the containers, use: docker-compose up –build\nThe application should now run in Docker containers and be accessible via the specified ports.\nMake sure that all services are configured correctly and the environment variables, if required, are adjusted in the docker-compose.yaml and .env files.",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Deployment</span>"
    ]
  },
  {
    "objectID": "userManual.html",
    "href": "userManual.html",
    "title": "User Manual",
    "section": "",
    "text": "1. First step: Log-in\nTo Log-in you are going to navigate form your Log-in screen to the Google-authentication field.",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>User Manual</span>"
    ]
  },
  {
    "objectID": "userManual.html#nice-to-know",
    "href": "userManual.html#nice-to-know",
    "title": "User Manual",
    "section": "Nice to know:",
    "text": "Nice to know:\nWhen you click on the Symbol of StratMystiqPro in the upper left croner you land on our HomePage. Here we give you a short description on what our tool does and how it works.",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>User Manual</span>"
    ]
  },
  {
    "objectID": "userManual.html#second-step-upload-reports",
    "href": "userManual.html#second-step-upload-reports",
    "title": "User Manual",
    "section": "2. Second step: Upload reports",
    "text": "2. Second step: Upload reports\nAfter you successfully logged-in you should land on the MyReports-Page where you can upload reports and see which reports you already have uploaded.  To upload a new report you have to click the button Upload new Report.  Then you can either choose or drag and drop a file into the field.  Afterwards you decide between checking the checkbox so the report is available for every user or not checking the box so the report is only be available for you.  After you uploaded the report you can klick on the SWOT field, this will guide you directly to the SWOT Page. If you want to delete a report you can do so over the traschcan symbol on the right side.",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>User Manual</span>"
    ]
  },
  {
    "objectID": "userManual.html#thrid-step-create-swot-analysis",
    "href": "userManual.html#thrid-step-create-swot-analysis",
    "title": "User Manual",
    "section": "3. Thrid step: Create SWOT-Analysis",
    "text": "3. Thrid step: Create SWOT-Analysis\n Through the dropdown-menu you can select a report from the uploaded reports. If you have already created a SWOT analysis for the selected report it will be automatically fetched from the database. Then you can see the summary first for each category:  If you then click into one of the four categories you will get a full description for every given point:  If you want to create a new SWOT-Analysis you can press the button Create SWOT.  If the system is creating the SWOT you will get a pop-up in the upper right croner with a message that the SWOT-Analysis will be created now.  After 5-6 minutes the SWOT-Analysis should be created and the summary should be visible in each category.",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>User Manual</span>"
    ]
  },
  {
    "objectID": "userManual.html#fourth-step-create-comparison-swot-analysis.",
    "href": "userManual.html#fourth-step-create-comparison-swot-analysis.",
    "title": "User Manual",
    "section": "4. Fourth step: Create Comparison-SWOT-Analysis.",
    "text": "4. Fourth step: Create Comparison-SWOT-Analysis.\nNow you landed on the Compare Companies Page. Here you can compare two bussiness reports from different companies with each other.  For this you can select two different reports in the drop-down menus. The same principle applies here as for the SWOT analysis. If you already created a Comparison-SWOT it will be fetched automatically after you selected the corresponding reports. The differences in each category will be listed in summary version first.  Then you can click into one of the category to see the full desricption for each difference.  If you want to create a new Comparison-SWOT you can click the button Create SWOT Comparison after you selected two reports.",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>User Manual</span>"
    ]
  },
  {
    "objectID": "userManual.html#fith-step-ask-the-chatbot-quastions-about-a-report.",
    "href": "userManual.html#fith-step-ask-the-chatbot-quastions-about-a-report.",
    "title": "User Manual",
    "section": "5. Fith step: Ask the ChatBot quastions about a report.",
    "text": "5. Fith step: Ask the ChatBot quastions about a report.\nOn the page Chatbot we created a Chatbot that is currently still work in progess and therefore an aplha version. The first thing you see is a disclaimer warning you that the Chatbot may give incorrect answers. You should be aware of this at any time you are using the Chatbot  The chatbot works the follwoing way: First you have to selecet a report that you have a question for. After this you can write the question in the text field and click send.  The system takes a few seconds to generate the answer so please be aware of this. Then you can see the answer to you question directly underneath it. The answer is based on the information given in the report.",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>User Manual</span>"
    ]
  },
  {
    "objectID": "userManual.html#sixth-step-log-out",
    "href": "userManual.html#sixth-step-log-out",
    "title": "User Manual",
    "section": "6. Sixth step: Log-out",
    "text": "6. Sixth step: Log-out\nWhen you have finished your analysis, you can log out by clicking on the icon in the top right-hand corner. From there you will be redirected back to the login screen.",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>User Manual</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Albert, M. 2016. Besseres Mobile-App-Design: Optimale Usability Für\niOS Und Android. entwickler.Press.\n\n\nArora, Daman, Anush Kini, Sayak Ray Chowdhury, Nagarajan Natarajan,\nGaurav Sinha, and Amit Sharma. 2023. “Gar-Meets-Rag Paradigm for\nZero-Shot Information Retrieval.” https://doi.org/10.48550/ARXIV.2310.20158.\n\n\nGao, Yunfan, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi,\nYi Dai, et al. 2023. “Retrieval-Augmented Generation for Large\nLanguage Models: A Survey.” https://doi.org/10.48550/ARXIV.2312.10997.\n\n\nLewis, Patrick, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich Küttler, et al. 2020.\n“Retrieval-Augmented Generation for Knowledge-Intensive Nlp\nTasks.” https://doi.org/10.48550/ARXIV.2005.11401.\n\n\nLi, Huayang, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022.\n“A Survey on Retrieval-Augmented Text Generation.” https://doi.org/10.48550/ARXIV.2202.01110.\n\n\nVite. n.d. “Vite Features.” Accessed January 10, 2024. https://vitejs.dev/guide/features.html.\n\n\nWang, Liang, Nan Yang, and Furu Wei. 2023. “Query2doc: Query\nExpansion with Large Language Models.” https://doi.org/10.48550/ARXIV.2303.07678.\n\n\nwebpack. n.d. “Hot Module Replacement.” Accessed January\n10, 2024. https://webpack.js.org/guides/hot-module-replacement/.",
    "crumbs": [
      "References"
    ]
  }
]