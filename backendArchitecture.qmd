# Backend Architecture
This part of the documentation will cover the backend architecture of the project.

## Backend Requirements
At the beginning of the project, we were trying to decide what technologies and architectures we should use to build our system.
To make these decisions, we decided to to define the requirements that the system should meet from a technical standpoint.
Seeing that the goal of this project is to build a prototype for a SaaS tool, the backend should be designed to
meet a number of requirements. These have been grouped into the following categories:

### Scalable

The system should be designed to handle multiple number of users and tasks at any time without impacting performance.

- **Handle multiple different Users at any time:**
  - Process a variety of reports simultaneously.
  - Provide answers to user queries efficiently.
- **Allow easy horizontal or vertical scalability:**
  - The system should be capable of scaling out (adding more nodes to the system) and scaling up (adding more power to existing nodes) as necessary, without any service interruption.

### Resource Efficient

Optimizing the use of resources is crucial for maintaining a cost-effective and high-performing system.

- **Minimize API-Calls to OpenAI and other external services:**
  - Reduce the frequency and volume of API requests to avoid unnecessary costs and to maintain system responsiveness.
- **Only regenerate responses if requested by a User:**
  - Cache and reuse responses where possible to avoid redundant computations.

### Adaptable

The system should be flexible to accommodate changes and the addition of new features.

- **Allow Users to upload reports from several business sectors:**
  - Support a diverse range of report formats and contents to cater to different industry needs.
- **It should be easy to add features to the tool without major code changes:**
  - Design the architecture to be modular, allowing new features to be added with minimal changes to the core codebase.

### Secure

Security is a top priority to protect user data and ensure trust in the system.

- **User Log-In should be secure and follow industry standards:**
  - Implement authentication mechanisms that meet current security best practices.
- **Ensure User Data is separated and protected from unauthorized access:**
  - Apply strict access controls and data isolation to ensure that user data is not exposed to unauthorized entities.


## Tech Stack
Based on the requirements defined above, we decided to use the following technologies to build the backend of the system.
The following sections will provide a brief overview of each technology and the reasons for choosing them.

### Python
The backend of the system is built using Python, some of the reasons for choosing Python are:

- **Ease of Use:**
  - All team members have experience working with Python, making it easy for all team members to actively contribute to the project.
- **Numerous Opensource Libraries:**
    - Python has a rich ecosystem of libraries and tools ranging from backend frameworks to machine learning libraries such as Flask and Langchain.

### FastAPI
FastAPI is a modern, fast (high-performance), web framework for building APIs with Python. The reasons for choosing FastAPI are:

- **Performance and Asynchronous Request Handling:**
    - FastAPI is built on top of Starlette for web routing and Pydantic for data validation, this makes it more performant than other Python web frameworks such as Flask.
    - It supports asynchronous request handling natively, which is crucial for handling multiple requests simultaneously where long-running tasks are expected.
- **Automatic API Documentation:**
    - FastAPI automatically generates API documentation based on the code, which makes it easier to maintain and update the documentation.
    - It provides a built-in interactive API documentation (Swagger UI) which makes it easier to test and debug the API. Without the need for additional tools such as Postman.

### Docker
Docker is a platform for developing, shipping, and running applications. The reasons for choosing Docker are:

- **Consistent Development Environment:**
    - Docker allows us to create a consistent environment for development as it greatly reduces the number of possible errors when working with different platforms (Mac and Windows).
    - Databases and other services can be run as containers, which makes it easier to set up and tear down the development environment quickly.
- **Easy Deployment:**
    - Docker containers can be deployed to any platform that supports Docker, which makes it easier to deploy the system to different cloud providers.
    - Services can be scaled horizontally by running multiple containers of the same service.

### MySQL
To store user, report and analysis data, we decided to use a relational database. The reasons for choosing MySQL are:

- **Experience:**
    - All team members have experience working with MySQL, making it easier for all team members to actively contribute to the project.
- **Scalability:**
    - Should the need arise, MySQL can be scaled horizontally by using sharding or clustering to handle a large number of users and reports.

### ChromaDB
ChromaDB is an open-source vector store that is designed to store and retrieve vector embeddings. The reasons for choosing ChromaDB are:

- **Natively Supported by Langchain:**
    - ChromaDB is natively supported by Langchain modules and thus requires minimal configuration to build initial prototypes.
- **Documentation and Community:**
    - ChromaDB has a rich documentation and an active community which makes it easier to get help and support when needed.

## Data Model
The data model of the application is designed to store user data, report data, and analysis data. The following sections will provide a brief overview of the data model and the relationships between the different entities.
The data model is designed to be flexible and adaptable to accommodate changes and the addition of new features.

### Overview Diagram
The following diagram provides an overview of the data model and the relationships between the different entities. As well
as the relationship between the relational database and vector store.

![Data Model of StratMystiqPro](images/DataModel.png){width=100% .lightbox}

### User
The User entity represents a user of the system. It is created when a user logs in to the system for the first time. The User entity has the following attributes:

- **user_id:** (PK:UUID) A unique identifier for the user.
- **username:** (String) The username of the user.
- **email:** (String) The email address of the user.
- **role:** (String) The role of the user, which can be either "admin" or "user".
- **created_at:** (DateTime) The date and time when the user was created.
- **updated_at:** (DateTime) The date and time when the user was last updated.

### Report
The Report entity is the central entity of the system. It represents a report that a user uploads to the system.
A Report by default is private and can only be accessed by the user who uploaded it. The User does have the option to make the report public.
The Report entity has the following attributes:

- **report_id:** (PK:UUID) A unique identifier for the report.
- **title:** (String) The title of the report.
- **filename:** (String) The filename of the report.
- **year:** (Integer) The year the report was published.
- **company_id:** (FK:UUID) The company identifier of the report. Foreign key to the Company entity.
- **report_public:** (Boolean) A flag to indicate if the report is public or private.
- **created_at:** (DateTime) The date and time when the report was created.
- **updated_at:** (DateTime) The date and time when the report was last updated.
- **deleted_at:** (DateTime) The date and time when the report was deleted.

### Company
The Company entity represents the company that the report is associated with. The Company entity has the following attributes:

- **company_id:** (PK:UUID) A unique identifier for the company.
- **name:** (String) The name of the company.
- **industry:** (String) The industry of the company.
- **created_at:** (DateTime) The date and time when the company was created.
- **updated_at:** (DateTime) The date and time when the company was last updated.

### Analysis
The Analysis entity is the object that contains the results of the main feature of the system. It represents the analysis
of a report that a user requests. Note that the Analysis entity is specifically not called SWOT Analysis, as the system
is designed to be flexible and adaptable to accommodate new types of analysis in the future. The Analysis entity has the following attributes:

- **analysis_id:** (PK:UUID) A unique identifier for the analysis.
- **report_id:** (FK:UUID) Reference to the report that the analysis is associated with.
- **compare_report_id:** (FK:UUID) If the analysis is a comparison analysis, this field will reference the report that was used for comparison.
- **type:** (String) The type of analysis, currently supported are "swot" or "swot_comparison".
- **content:** (Text) The content of the analysis, which is a JSON string that contains the results of the analysis.
- **created_at:** (DateTime) The date and time when the analysis was created.
- **updated_at:** (DateTime) The date and time when the analysis was last updated.

### Analysis Section
The Analysis Section entity represents a section of the analysis. It was introduced towards the end of the project to store
intermediate results of the analysis. In case an analysis process is interrupted, the system can use the intermediate results
to continue the analysis where it stopped. The Analysis Section entity has the following attributes:

- **partial_result_id:** (PK:UUID) A unique identifier for the partial result.
- **analysis_id:** (FK:UUID) Reference to the analysis that the partial result is associated with.
- **section:** (String) The section of the analysis, which can be "strengths", "weaknesses", "opportunities" or "threats".
- **content:** (Text) The content of the partial result, which is a JSON string that contains the intermediate results of the analysis.
- **created_at:** (DateTime) The date and time when the partial result was created.
- **updated_at:** (DateTime) The date and time when the partial result was last updated.

### Screenshot
The Screenshot entity is used to track and identify the screenshots that are taken of the reports. The Screenshots are either
the identified tables or the cover page of the report. It is implemented in a way that allows all other types of screenshots
to be added in the future. The Screenshot entity has the following attributes:

- **screenshot_id:** (PK:UUID) A unique identifier for the screenshot.
- **report_id:** (FK:UUID) Reference to the report that the screenshot is associated with.
- **filename:** (String) The filename of the screenshot.
- **type:** (String) The type of the screenshot, which can be "table" or "image".
- **description:** (Text) A JSON string that contains the information extracted from the screenshot.
- **created_at:** (DateTime) The date and time when the screenshot was created.
- **updated_at:** (DateTime) The date and time when the screenshot was last updated.

### Job
The Job entity was originally introduced to track the status of the report ingestion. While developing the system,
we realized that we need to track the status of many long-running task in the system. It also acts a Job-List for the
Backend to process outstanding jobs. The Job entity has the following attributes:

- **job_id:** (PK:UUID) A unique identifier for the job.
- **object_id:** (UUID) Reference to the object that the job is associated with. Currently supported are report_id, analysis_id or screenshot_id.
- **object_type:** (String) The type of the object that the job is associated with. Currently supported are "report", "analysis" or "screenshot".
- **user_id:** (FK:UUID) Reference to the user that the job is associated with.
- **status:** (String) The status of the job, which can be "pending", "in_progress", "completed" or "failed".
- **percentage:** (Integer) The percentage of the job that has been completed. (Only supported in report_ingestion)
- **request_date:** (DateTime) The date and time when the job was requested.
- **completion_date:** (DateTime) The date and time when the job was completed.
- **created_at:** (DateTime) The date and time when the job was created.
- **updated_at:** (DateTime) The date and time when the job was last updated.

### Vector Store
In the report ingestion process, the system extracts the text from the report, splits it into chunks and stores it in the vector store. The vector store
is used to store the vector embeddings of the text, the text that was embedded, as well as metadata to identify the individual chunks.
The Vector Store knows the following two entities:

#### Collection
The Collection entity represents a group of vector embeddings. We currently only use one collection to store the embeddings of all reports.
For future use cases, we planned on adding more collections to store embeddings of different types of data (Websites...)
The Collection entity has the following attributes:

- **collection_id:** (PK:UUID) A unique identifier for the collection.
- **name:** (String) The name of the collection.
- **created_at:** (DateTime) The date and time when the collection was created.

#### Chunk
The Chunk entity represents a single chunk of text that was embedded and stored in the vector store. The Chunk entity has the following attributes:

- **chunk_id:** (PK:UUID) A unique identifier for the chunk.
- **vectors:** (Array) An array of floats that represents the vector embeddings of the text.
- **text:** (Text) The text that was embedded.
- **metadata:** (JSON) A JSON string that contains metadata to identify the chunk.
    - **report_id:** (UUID) The report the chunk is associated with. Links to the Report entity in the relational database.
    - **chunk_type:** (String) The type of the chunk, which can be "table" or "report_text".
    - **extra_content:** (JSON) Only used for "table" chunks. Contains the extracted table data.
    - **page_number:** (Integer) The page number of the report where the chunk was extracted from.


## Python Classes
The following section will focus on the Python classes that were implemented to handle the business logic of the system.
It should be noted that this structure was not implemented from the beginning of the project, but has evolved over time as the system grew in complexity.
Specifically, the classes that represent the entities of the system such as User, Report, Analysis, etc. don't contain any business logic
and are only used to represent the data model of the system. They are needed to interact with the relational database using the SQLAlchemy ORM.


Furthermore, certain areas of the system such as the implementation of the FastAPI endpoints are not programmed in a class-based
structure, but rather in a function-based structure as recommended by the FastAPI documentation. The following sections
will provide a brief overview of the relevant classes and their responsibilities.

### Class Diagram
The following diagram provides an overview of the classes and their relationships. The diagram does not contain the
FastAPI endpoints, and authentication logic as they are not implemented as classes.
![Class Diagram](images/ClassDiagram.png){width=100% .lightbox}

### Config
The Config class is responsible for loading environment variables, initializing the OpenAI API, the Chroma Vector Store,
and the MySQL Database. It does this in its init method. The environment variables are loaded from a .env file or from
the Docker environment, depending on the environment in which the application is running.

The Config class is used to manage the configuration of the application. It provides a centralized place to manage all
configuration settings. This makes it easier to change settings and manage dependencies as they are all in one place.

The Config class is written as a Singleton class. The Singleton pattern is a design pattern that
restricts the instantiation of a class to a single instance. This is important as having multiple instances of the Config
class could lead to inconsistencies in the configuration settings across different parts of the application.
Furthermore, the Config class is responsible for initializing several dependencies such as the database connections.
These resources should be shared across the application and not duplicated.

### Database

### ChromaConnector

### BusinessAnalyst

## Authentication

## REST API



